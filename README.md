# LLM-Based Triaging Pipeline for Android Native Code Vulnerabilities

## Table of Contents
- [Scope](#scope)
- [Components](#components)
- [Dependencies](#dependencies)
- [Installation](#installation)
  - [Manual Installation](#manual-installation)
  - [Docker Installation](#docker-installation)
- [Configuration](#configuration)
- [Usage](#usage)
  - [Parameters](#parameters)
  - [Examples](#examples)
- [Output](#output)

---

## Scope

This project implements an **LLM-based triaging pipeline** for automatically assessing the security risk of Android applications that integrate native code (C/C++) via the Java Native Interface (JNI). It is part of a master's degree thesis focused on automatic triaging of APKs.

### Background

Android applications often integrate native code for improved performance or specific features. However, when apps use JNI to call native code, they can be exposed to vulnerabilities in the native layer. Unlike Java and Kotlin, C/C++ lacks memory-safety guarantees, making issues such as buffer overflows, use-after-free errors, and other memory corruption problems more likely.

### Approach

This pipeline uses crash reports generated by **POIROT** (a tool that automatically generates harnesses and fuzzes native libraries) and leverages Large Language Models (LLMs) to automatically estimate whether a crash indicates a real vulnerability.

The system uses the **Model Context Protocol (MCP)** to provide the LLM with controlled access to reverse-engineering tools:
- **Jadx**: For Android bytecode and manifest analysis
- **Ghidra**: For native disassembly and decompilation

For each crash, the pipeline:
1. Analyzes the origin of the crash
2. Returns a classification stating whether the issue is likely a vulnerability
3. Provides a confidence level for the judgment
4. Explains the reasoning in plain terms with concrete evidence (stack frames, function names, decompiled snippets)
5. Indicates severity level
6. Suggests practical next steps or mitigations
7. Explicitly notes assumptions, limitations, and missing information

**Goal**: Reduce analyst effort and improve consistency by automating crash triage with verifiable context from MCP tools.

---

## Components

This pipeline integrates several key components:

### 1. **Model Context Protocol (MCP)**
The MCP framework enables controlled access to reverse-engineering tools, allowing the LLM to query specific information from Jadx and Ghidra without exposing the entire codebase.

### 2. **Jadx MCP Server**
- **Repository**: [jadx-ai-mcp](https://github.com/zinja-coder/jadx-ai-mcp)
- **Purpose**: Provides the LLM with access to Android bytecode, manifest files, and Java/Kotlin code context
- **Capabilities**: Extract app metadata (package name, SDK versions, permissions), analyze Java code structure, and identify JNI method declarations

### 3. **Ghidra MCP Server**
- **Repository**: [GhidraMCP](https://github.com/LaurieWired/GhidraMCP)
- **Purpose**: Provides the LLM with access to native library disassembly and decompilation
- **Capabilities**: Analyze native functions, identify memory operations, examine stack frames, and provide decompiled C/C++ code

### 4. **POIROT Integration**
- **Repository**: [droidot](https://github.com/HexHive/droidot/)
- **Purpose**: Generates crash reports through automated fuzzing of native libraries
- **Output**: The pipeline expects POIROT's crash reports as input (specifically `folder2backtraces.txt` files)

---

## Dependencies

### System Requirements
- **Python**: 3.13.3 (or compatible version)
- **Java**: OpenJDK 17 or 21
- **Operating System**: Linux (Ubuntu 24.04 recommended)
- **Display Server**: X11 (for Jadx GUI)

### Python Packages
All Python dependencies are listed in `requirements.txt`:
- `requests` - HTTP library
- `mcp` - Model Context Protocol core
- `pyautogui` - GUI automation for Jadx
- `llama-index-tools-mcp` - MCP integration tools
- `openai` - OpenAI API client
- `xmltodict` - XML parsing for Android manifests
- `python-dotenv` - Environment variable management
- `pydantic-ai-slim[google,mcp,openai]` - LLM framework with MCP support

### External Tools
- **Jadx** (v1.5.3): Android decompiler
- **Ghidra** (v11.4.2): Reverse engineering framework
- **Android SDK**: For APK analysis
- **Maven**: For building FlowDroid call graph generator
- **Node.js** (v20): For Gemini CLI (optional)

---

## Installation

### Manual Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd LLM-Triaging
   ```

2. **Install system dependencies**:
   ```bash
   sudo apt update
   sudo apt install -y openjdk-17-jdk python3 python3-pip python3-tk python3-dev \
                       wget unzip maven android-sdk wmctrl \
                       libcanberra-gtk-module libcanberra-gtk3-module
   touch .Xauthority # Needed for pyautogui
   ```

3. **Install Jadx**:
   ```bash
   wget https://github.com/skylot/jadx/releases/download/v1.5.3/jadx-1.5.3.zip
   unzip jadx-1.5.3.zip -d /opt/jadx
   export PATH="/opt/jadx/bin:$PATH"
   ```

4. **Install Ghidra**:
   ```bash
   wget https://github.com/NationalSecurityAgency/ghidra/releases/download/Ghidra_11.4.2_build/ghidra_11.4.2_PUBLIC_20250826.zip
   unzip ghidra_11.4.2_PUBLIC_20250826.zip -d /opt/ghidra
   export PATH="/opt/ghidra:$PATH"
   ```

5. **Set up MCP servers**:
   
   **Jadx MCP**:
   ```bash
   mkdir -p MCP_servers
   cd MCP_servers
   wget https://github.com/zinja-coder/jadx-ai-mcp/releases/download/v4.0.0/jadx-mcp-server-v4.0.0.zip
   unzip jadx-mcp-server-v4.0.0.zip -d jadx-mcp-server
   
   # Install Jadx AI plugin
   jadx plugins --install "github:zinja-coder:jadx-ai-mcp"
   ```
   
   **Ghidra MCP**:
   ```bash
   wget https://github.com/LaurieWired/GhidraMCP/releases/download/1.4/GhidraMCP-release-1-4.zip
   unzip GhidraMCP-release-1-4.zip
   mv GhidraMCP-release-1-4 GhidraMCP
   cd ..
   ```

6. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

7. **Build FlowDroid call graph generator** (optional):
   ```bash
   cd flowdroid_gen/flowdroid-cg
   mvn clean package
   cd ../..
   ```

### Docker Installation

The easiest way to run the pipeline is using Docker, which includes all dependencies pre-configured.

1. **Build the Docker image**:
   ```bash
   docker build -t llm-triaging .
   ```

2. **Run the container**:
   ```bash
   docker run -it --rm \
     -v $(pwd):/workspace \
     -e DISPLAY=$DISPLAY \
     -v /tmp/.X11-unix:/tmp/.X11-unix \
     llm-triaging
   ```

   > **Note**: The `-e DISPLAY` and `-v /tmp/.X11-unix` flags enable X11 forwarding for Jadx GUI. You may need to run `xhost +local:docker` on your host machine to allow Docker containers to access the display.

3. **Configure environment variables** (see [Configuration](#configuration) section)

---

## Configuration

The pipeline requires several environment variables to be configured. Create a `.env` file in the project root with the following variables:

```bash
# MCP Server Paths
JADX_MCP_DIR="MCP_servers/jadx-mcp-server"
GHIDRA_MCP_DIR="MCP_servers/GhidraMCP"
GHIDRA_INSTALL_DIR="/opt/ghidra/ghidra_11.4.2_PUBLIC/"

# LLM Configuration
LLM_API_KEY=your-api-key-here
LLM_MODEL_NAME=gpt-5
OLLAMA_URL=http://localhost:11434/v1
```

### Configuration Parameters

| Variable | Description | Example |
|----------|-------------|---------|
| `JADX_MCP_DIR` | Path to Jadx MCP server directory | `MCP_servers/jadx-mcp-server` |
| `GHIDRA_MCP_DIR` | Path to Ghidra MCP server directory | `MCP_servers/GhidraMCP` |
| `GHIDRA_INSTALL_DIR` | Path to Ghidra installation | `/opt/ghidra/ghidra_11.4.2_PUBLIC/` |
| `LLM_API_KEY` | API key for your LLM provider (OpenAI, Google, etc.) | `sk-...` or `AIza...` |
| `LLM_MODEL_NAME` | Name of the LLM model to use | `gpt-5`, `gemini-2.5-flash`, etc. |
| `OLLAMA_URL` | Base URL for Ollama (if using local models) | `http://localhost:11434/v1` |

> **Important**: For Docker installations, the paths should be adjusted to match the container's filesystem structure (e.g., `/workspace/MCP_servers/...`).

---

## Usage

### Basic Command

```bash
python3 main.py <POIROT_OUTPUT_DIR> [OPTIONS]
```

### Parameters

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `target_APK` | Path | Path to POIROT output directory containing APK subdirectories | *Required* |
| `--apk-list` | Path | Path to a `.txt` file with APK/app names to include (one per line) | None |
| `-m, --model-name` | String | LLM model name | `$LLM_MODEL_NAME` or `gpt-5` |
| `-s, --ollama-url` | String | Base URL for Ollama requests | `$OLLAMA_URL` or `http://localhost:11434/v1` |
| `-o, --out-dir` | Path | Output directory for reports | `classification_YYYY_MM_DD_HH:MM` |
| `--timeout` | Integer | Timeout in seconds for MCP servers | `180` |
| `--flowgraph-dir` | Path | Directory for call graphs from FlowDroid | `callGraph` |
| `-d, --debug` | Flag | Enable verbose debug logs | `False` |
| `-v, --verbose` | Flag | Echo prompts and model outputs | `False` |

### Input Structure

The pipeline expects POIROT output in the following structure:

```
POIROT_OUTPUT_DIR/
├── APPNAME_1/
│   ├── base.apk
│   └── fuzzing_output/
│       └── <case_dir>/
│           └── reproduced_crashes/
│               └── folder2backtraces.txt
├── APPNAME_2/
│   ├── base.apk
│   └── fuzzing_output/
│       └── <case_dir>/
│           └── reproduced_crashes/
│               └── folder2backtraces.txt
...
```

> **Note**: `<case_dir>` names must match the pattern: `fname-signature@cs_number-io_matching_possibility`

### Examples

1. **Process all APKs in POIROT output**:
   ```bash
   python3 main.py /path/to/poirot_output -d
   ```

2. **Process specific APKs from a list**:
   ```bash
   python3 main.py /path/to/poirot_output --apk-list apps_to_analyze.txt
   ```

3. **Use a specific model with custom output directory**:
   ```bash
   python3 main.py /path/to/poirot_output \
     -m gemini-2.5-flash \
     -o results/my_analysis \
     -v
   ```

4. **Use Ollama with a local model**:
   ```bash
   python3 main.py /path/to/poirot_output \
     -m llama3.2 \
     -s http://localhost:11434/v1 \
     --debug
   ```

---

## Output

The pipeline generates structured JSON reports for each analyzed crash. Reports are organized as follows:

```
<out-dir>/
├── APPNAME_1/
│   └── <case_dir>/
│       └── report.json
├── APPNAME_2/
│   └── <case_dir>/
│       └── report.json
...
```

### Report Structure

Each `report.json` contains:

```json
{
  "analysis": {
    "tool": {
      "model_name": "gpt-5.1",
      "apk_path": "/path/to/base.apk",
      "version": "1.0"
    },
    "app": {
      "package_name": "com.example.app",
      "app_label": "Example App",
      "min_sdk": 21,
      "target_sdk": 34,
      ...
    },
    "analysisResults": [
      {
         "crash": { 
            "StackTrace": [...],
            "JavaCallGraph": [...],
            "LibMap": { "...": [...] }
         }
         "assessment": {
          "chain_of_thought": [
            "Reasoning step 1...",
            "Reasoning step 2...",
            "..."
          ],
          "is_vulnerable": true/false,
         }
         "is_vulnerable": true,
         "confidence": [0-1],
         "cwe_ids": [...],
         "severity": "LOW-HIGH",
         "reasons": [...],
         "evidence": [...],
         "assumptions": [...],
         "limitations": [...],
         ...
         "exploit": {
            ...
            "prerequisites": [...]
            "exploit_pipeline": [...]
            "poc_commands": [...]
            ...
         }
      }
    ]
  }
}
```

### Key Fields

- **is_vulnerable**: Boolean indicating if the crash likely represents a vulnerability
- **confidence**: Float between 0 and 1 indicating the model's confidence
- **severity**: Risk level (e.g., `LOW`, `MEDIUM`, `HIGH`, `CRITICAL`)
- **reasons**: List of explanations for the classification
- **evidence**: Concrete evidence (stack frames, function names, code snippets)
- **next_steps**: Suggested actions for further investigation or mitigation
- **assumptions**: Assumptions made during analysis
- **limitations**: What information was missing or uncertain

---

## License

This project is part of a master's degree thesis. Please refer to the repository for licensing information.

## Acknowledgments

- [POIROT/droidot](https://github.com/HexHive/droidot/) - Automated fuzzing framework
- [Jadx AI MCP](https://github.com/zinja-coder/jadx-ai-mcp) - Jadx MCP server
- [GhidraMCP](https://github.com/LaurieWired/GhidraMCP) - Ghidra MCP server
